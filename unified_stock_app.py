"""
Unified Stock Forecasting App
Single app with sync + analysis + web interface
"""
from flask import Flask, render_template, jsonify, request, session, redirect, url_for
from flask_cors import CORS
from dotenv import load_dotenv
import sqlite3
import requests
import time
import os
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional
import secrets

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Set secret key for sessions
app.secret_key = os.environ.get('FLASK_SECRET_KEY', secrets.token_hex(16))

# Password protection
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD')

def require_auth(f):
    """Decorator to require password authentication"""
    def decorated_function(*args, **kwargs):
        # Skip auth if no password is set (development)
        if not ADMIN_PASSWORD:
            return f(*args, **kwargs)
        
        # Check if user is authenticated
        if not session.get('authenticated'):
            return redirect(url_for('login'))
        
        return f(*args, **kwargs)
    decorated_function.__name__ = f.__name__
    return decorated_function

class UnifiedCin7Client:
    """Unified Cin7 client with sync and analysis capabilities"""
    
    def __init__(self):
        self.account_id = os.environ.get('CIN7_ACCOUNT_ID')
        self.api_key = os.environ.get('CIN7_API_KEY')
        self.base_url = os.environ.get('CIN7_BASE_URL', 'https://inventory.dearsystems.com/ExternalApi/v2')
        
        if not self.account_id or not self.api_key:
            raise ValueError("Missing CIN7_ACCOUNT_ID or CIN7_API_KEY")
        
        self.last_request_time = 0
        self.min_interval = 1.8
        
        self.init_database()
    
    def init_database(self):
        """Initialize database"""
        # Use persistent disk path in production, local path in development
        db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
        
        # If no custom path set, check for Render persistent disk
        if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
            db_path = '/data/db/stock_forecast.db'
        
        # Ensure directory exists for persistent storage
        if db_path != 'stock_forecast.db':
            os.makedirs(os.path.dirname(db_path), exist_ok=True)
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS products (
                id INTEGER PRIMARY KEY,
                sku TEXT UNIQUE NOT NULL,
                description TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS orders (
                id INTEGER PRIMARY KEY,
                order_number TEXT NOT NULL,
                sku TEXT NOT NULL,
                quantity REAL NOT NULL,
                warehouse TEXT NOT NULL,
                booking_date TEXT NOT NULL,
                reference_id TEXT UNIQUE NOT NULL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _make_request(self, endpoint: str, params: Dict = None):
        """Rate-limited API request"""
        # Rate limiting
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        
        headers = {
            'api-auth-accountid': self.account_id,
            'api-auth-applicationkey': self.api_key,
            'Content-Type': 'application/json'
        }
        
        url = f"{self.base_url}{endpoint}"
        response = requests.get(url, headers=headers, params=params, timeout=30)
        self.last_request_time = time.time()
        
        if response.status_code == 429:
            time.sleep(60)
            return self._make_request(endpoint, params)
        
        response.raise_for_status()
        return response.json()
    
    def sync_date_window(self, start_date: str, end_date: str, max_orders: int = 50):
        """Sync orders for specific date window"""
        try:
            logger.info(f"ðŸ”„ Syncing {start_date} to {end_date}")
            
            # Get orders
            params = {
                'Page': 1,
                'Limit': 1000,
                'OrderDateFrom': start_date,
                'OrderDateTo': end_date
            }
            
            result = self._make_request('/SaleList', params)
            orders = result.get('SaleList', [])[:max_orders]
            
            db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
            
            # If no custom path set, check for Render persistent disk
            if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
                db_path = '/data/db/stock_forecast.db'
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            stored_lines = 0
            
            for order in orders:
                if order.get('Status', '').upper() == 'VOIDED':
                    continue
                
                # Get order detail
                try:
                    detail = self._make_request('/Sale', {'ID': order.get('SaleID')})
                    
                    # Extract lines using example app's proven logic
                    # Priority 1: Pick lines (from fulfilments)
                    pick_lines = []
                    fulfilments = detail.get('Fulfilments', [])
                    
                    for fulfilment in fulfilments:
                        pick_data = fulfilment.get('Pick', {})
                        if pick_data.get('Lines'):
                            for line in pick_data['Lines']:
                                pick_lines.append({
                                    'sku': line.get('SKU', '').strip(),
                                    'quantity': line.get('Quantity', 0),
                                    'location': line.get('Location', ''),
                                    'description': line.get('Name', '')
                                })
                    
                    # Priority 2: Order lines (fallback)
                    order_lines = []
                    order_data = detail.get('Order', {})
                    if order_data.get('Lines'):
                        for line in order_data['Lines']:
                            order_lines.append({
                                'sku': line.get('SKU', '').strip(),
                                'quantity': line.get('Quantity', 0),
                                'location': None,
                                'description': line.get('Name', '')
                            })
                    
                    # Use pick lines if available, otherwise order lines (like example app)
                    lines_to_process = pick_lines if pick_lines else order_lines
                    
                    logger.info(f"   ðŸ“¦ Order {order.get('OrderNumber')}: {len(pick_lines)} pick lines, {len(order_lines)} order lines")
                    logger.info(f"   âœ… Processing {len(lines_to_process)} lines from {'pick' if pick_lines else 'order'} source")
                    
                    for line in lines_to_process:
                        sku = line['sku']
                        quantity = line['quantity']
                        description = line['description']
                        location = line['location']
                        
                        if not sku or quantity <= 0:
                            continue
                        
                        # Map warehouse from location (like example app)
                        warehouse = 'NSW'  # Default
                        if location:
                            if 'CNTVIC' in location or 'VIC' in location:
                                warehouse = 'VIC'
                            elif 'WCLQLD' in location or 'QLD' in location:
                                warehouse = 'QLD'
                        
                        reference_id = f"{order.get('SaleID')}:{sku}"
                        
                        # Check if exists
                        cursor.execute('SELECT id FROM orders WHERE reference_id = ?', (reference_id,))
                        if cursor.fetchone():
                            continue
                        
                        # Add product if needed
                        cursor.execute('INSERT OR IGNORE INTO products (sku, description) VALUES (?, ?)', 
                                     (sku, description))
                        
                        # Add order
                        cursor.execute('''
                            INSERT INTO orders 
                            (order_number, sku, quantity, warehouse, booking_date, reference_id)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            order.get('OrderNumber', ''),
                            sku,
                            quantity,
                            warehouse,
                            order.get('OrderDate', '').split('T')[0],
                            reference_id
                        ))
                        
                        stored_lines += 1
                    
                except Exception as e:
                    logger.error(f"Failed to process order: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            return {
                'success': True,
                'orders_found': len(orders),
                'lines_stored': stored_lines
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def sync_stock_from_cin7(self):
        """Sync real stock levels from Cin7 /ref/ProductAvailability endpoint"""
        try:
            logger.info("ðŸ”„ Syncing real stock levels from Cin7...")

            # Fetch stock data from Cin7
            all_stock = {}
            page = 1
            
            while page <= 10:  # Safety limit
                params = {
                    'Page': page,
                    'Limit': 1000
                }
                
                logger.info(f"ðŸ“‹ Fetching stock page {page}...")
                
                result = self._make_request('/ref/ProductAvailability', params)
                items = result.get('ProductAvailabilityList', [])
                
                if not items:
                    logger.info(f"ðŸ“„ Page {page}: No more stock data")
                    break
                
                logger.info(f"ðŸ“„ Page {page}: {len(items)} stock records")
                
                # Get selected SKUs for filtering (or all if none selected)
                selected_skus = get_selected_skus()
                
                if selected_skus:
                    # Filter for selected SKUs only
                    filtered_items = [item for item in items if 
                                    str(item.get('SKU', '')) in selected_skus]
                    logger.info(f"ðŸŽ¯ Found {len(filtered_items)} selected SKUs on page {page}")
                else:
                    # If no selection, get all items
                    filtered_items = items
                    logger.info(f"ðŸ“¦ Found {len(filtered_items)} total items on page {page}")
                
                # Aggregate stock by SKU across all locations
                for item in filtered_items:
                    sku = item.get('SKU', '')
                    on_hand = float(item.get('OnHand', 0))
                    
                    if sku not in all_stock:
                        all_stock[sku] = 0
                    
                    all_stock[sku] += on_hand
                
                if len(items) < 1000:
                    break
                    
                page += 1
                time.sleep(1.2)  # Rate limiting
            
            logger.info(f"âœ… Stock sync complete: {len(all_stock)} SKUs found")
            
            return {
                'success': True,
                'stock_levels': all_stock,
                'sku_count': len(all_stock),
                'method': '/ref/ProductAvailability'
            }
            
        except Exception as e:
            logger.error(f"Stock sync failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def sync_recent_orders(self, created_since: str, max_orders: int = 500):
        """Sync recent orders using CreatedSince (the working date filter)"""
        try:
            logger.info(f"ðŸš€ Starting OPTIMIZED recent orders sync since {created_since}")

            db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
            
            # If no custom path set, check for Render persistent disk
            if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
                db_path = '/data/db/stock_forecast.db'
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # OPTIMIZATION 1: Preload existing orders to skip API calls
            logger.info("âš¡ Preloading existing orders...")
            cursor.execute('SELECT DISTINCT reference_id FROM orders WHERE reference_id IS NOT NULL')
            existing_orders = {row[0] for row in cursor.fetchall()}
            logger.info(f"ðŸ“‹ Found {len(existing_orders)} existing orders to skip")

            # Track results
            orders_found = 0
            stored_lines = 0
            skipped_existing = 0
            batch_data = []  # For batch inserts

            # Fetch orders using CreatedSince
            page = 1
            total_fetched = 0

            while page <= 20 and total_fetched < max_orders:  # Safety limits
                # OPTIMIZATION 2: Use maximum page size
                params = {
                    'Page': page,
                    'Limit': 1000,  # Maximum page size for speed
                    'CreatedSince': created_since
                }

                logger.info(f"ðŸ“‹ Fetching page {page} with CreatedSince (limit: 1000)...")

                result = self._make_request('/SaleList', params)
                orders = result.get('SaleList', [])

                if not orders:
                    logger.info(f"ðŸ“„ Page {page}: No more orders")
                    break

                orders_found += len(orders)
                total_fetched += len(orders)

                logger.info(f"ðŸ“„ Page {page}: {len(orders)} orders")

                # OPTIMIZATION 3: Filter orders before expensive detail calls
                orders_to_detail = []
                
                for order in orders:
                    if order.get('Status', '').upper() == 'VOIDED':
                        continue

                    sale_id = order.get('SaleID')
                    if not sale_id:
                        continue

                    # Skip existing orders (avoid API call)
                    if sale_id in existing_orders:
                        skipped_existing += 1
                        continue

                    orders_to_detail.append(order)

                logger.info(f"   ðŸ“Š Processing {len(orders_to_detail)} new orders (skipped {skipped_existing} existing)")

                # Process only new orders
                for order in orders_to_detail:
                    sale_id = order.get('SaleID')
                    
                    try:
                        detail = self._make_request('/Sale', {'ID': sale_id})

                        # Extract lines using example app's proven logic
                        # Priority 1: Pick lines (from fulfilments)
                        pick_lines = []
                        fulfilments = detail.get('Fulfilments', [])

                        for fulfilment in fulfilments:
                            pick_data = fulfilment.get('Pick', {})
                            if pick_data.get('Lines'):
                                for line in pick_data['Lines']:
                                    pick_lines.append({
                                        'sku': line.get('SKU', '').strip(),
                                        'quantity': line.get('Quantity', 0),
                                        'location': line.get('Location', ''),
                                        'description': line.get('Name', '')
                                    })

                        # Priority 2: Order lines (fallback)
                        order_lines = []
                        order_data = detail.get('Order', {})
                        if order_data.get('Lines'):
                            for line in order_data['Lines']:
                                order_lines.append({
                                    'sku': line.get('SKU', '').strip(),
                                    'quantity': line.get('Quantity', 0),
                                    'location': None,
                                    'description': line.get('Name', '')
                                })

                        # Use pick lines if available, otherwise order lines
                        lines_to_process = pick_lines if pick_lines else order_lines

                        logger.info(f"   ðŸ“¦ Order {order.get('OrderNumber')}: {len(pick_lines)} pick lines, {len(order_lines)} order lines")

                        # OPTIMIZATION 4: Batch data collection instead of individual inserts
                        for line in lines_to_process:
                            sku = line['sku']
                            quantity = line['quantity']
                            location = line['location']

                            if not sku or quantity <= 0:
                                continue

                            # Map warehouse from location (improved logic)
                            warehouse = 'NSW'  # Default
                            if location:
                                if 'CNTVIC' in location or 'VIC' in location:
                                    warehouse = 'VIC'
                                elif 'WCLQLD' in location or 'QLD' in location:
                                    warehouse = 'QLD'

                            # Create unique reference_id for this line
                            reference_id = f"{sale_id}:{sku}"

                            # Add to batch instead of immediate insert
                            batch_data.append((
                                order.get('OrderNumber', ''),
                                sku,
                                quantity,
                                warehouse,
                                order.get('OrderDate', '').split('T')[0],
                                reference_id
                            ))

                    except Exception as e:
                        logger.error(f"Error processing order {sale_id}: {e}")
                        continue

                # OPTIMIZATION 5: Batch insert after processing page
                if batch_data:
                    # Add products first (batch)
                    product_batch = list(set([(item[1], item[1]) for item in batch_data]))  # sku, description
                    cursor.executemany('INSERT OR IGNORE INTO products (sku, description) VALUES (?, ?)', 
                                     product_batch)
                    
                    # Add orders (batch)
                    cursor.executemany('''
                        INSERT OR IGNORE INTO orders 
                        (order_number, sku, quantity, warehouse, booking_date, reference_id)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', batch_data)
                    
                    stored_lines += len(batch_data)
                    conn.commit()  # Commit after each page
                    logger.info(f"   ðŸ’¾ Batch inserted {len(batch_data)} lines from page {page}")
                    batch_data = []

                if len(orders) < 1000:
                    break

                page += 1
                time.sleep(1.2)  # Rate limiting between pages

            conn.close()

            logger.info(f"âœ… OPTIMIZED sync complete: {stored_lines} lines from {orders_found} orders (skipped {skipped_existing} existing)")

            return {
                'success': True,
                'orders_found': orders_found,
                'lines_stored': stored_lines,
                'skipped_existing': skipped_existing,
                'created_since': created_since,
                'method': 'CreatedSince_Optimized'
            }

        except Exception as e:
            logger.error(f"Recent sync failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }


# Initialize client
cin7_client = UnifiedCin7Client()

def get_db():
    # Use same database path as the client
    db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
    
    # If no custom path set, check for Render persistent disk
    if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
        db_path = '/data/db/stock_forecast.db'
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn

@app.route('/login', methods=['GET', 'POST'])
def login():
    """Simple password login"""
    # If no password is set, redirect to dashboard
    if not ADMIN_PASSWORD:
        return redirect(url_for('dashboard'))
    
    if request.method == 'POST':
        password = request.form.get('password')
        if password == ADMIN_PASSWORD:
            session['authenticated'] = True
            return redirect(url_for('dashboard'))
        else:
            return render_template('login.html', error='Invalid password')
    
    return render_template('login.html')

@app.route('/logout')
def logout():
    """Logout and clear session"""
    session.clear()
    return redirect(url_for('login'))

@app.route('/')
@require_auth
def dashboard():
    """Redirect to reorder page (main functionality)"""
    return redirect(url_for('reorder_dashboard'))

@app.route('/reorder')
@require_auth
def reorder_dashboard():
    """Enhanced reorder dashboard with mathematical explanations"""
    return render_template('enhanced_reorder_dashboard.html')

@app.route('/sku-management')
@require_auth
def sku_management():
    """SKU management page for selecting which SKUs to analyze"""
    return render_template('sku_management.html')

@app.route('/api/dashboard/status')
@require_auth
def dashboard_status():
    """Get current status"""
    try:
        conn = get_db()
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM products')
        total_skus = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM orders')
        total_orders = cursor.fetchone()[0]
        
        cursor.execute('SELECT MIN(booking_date) as min_date, MAX(booking_date) as max_date FROM orders')
        date_range = cursor.fetchone()
        
        conn.close()
        
        return jsonify({
            'total_skus': total_skus,
            'total_orders': total_orders,
            'data_range': {
                'from': date_range['min_date'],
                'to': date_range['max_date']
            } if date_range['min_date'] else None
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/sync/window')
@require_auth
def sync_window():
    """Sync specific date window"""
    start_date = request.args.get('start', '2025-09-01')
    end_date = request.args.get('end', '2025-09-24')
    max_orders = int(request.args.get('max', 20))
    
    result = cin7_client.sync_date_window(start_date, end_date, max_orders)
    return jsonify(result)

@app.route('/api/sync/comprehensive')
@require_auth
def sync_comprehensive():
    """Comprehensive historical sync using CreatedSince approach"""
    try:
        logger.info("ðŸš€ Starting comprehensive historical sync using CreatedSince")
        
        from datetime import datetime, timedelta
        import pytz
        import time
        
        # GMT+10 timezone
        tz = pytz.timezone('Australia/Sydney')
        now_local = datetime.now(tz)
        
        # Define sync batches by days back (CreatedSince approach)
        # Start recent and work backwards
        batches = [
            (30, 'Last 30 days', 1000),
            (90, 'Last 3 months', 1500), 
            (180, 'Last 6 months', 2000),
            (365, 'Last year', 2500),
            (730, 'Last 2 years', 3000)
        ]
        
        total_orders = 0
        total_lines = 0
        batch_results = []
        last_created_since = None
        
        for i, (days_back, label, max_orders) in enumerate(batches):
            # Skip if this batch overlaps with previous
            if last_created_since and days_back <= last_created_since:
                continue
                
            logger.info(f"ðŸ“… Syncing batch {i+1}/{len(batches)}: {label}")
            
            try:
                # Calculate CreatedSince date
                since_date = now_local - timedelta(days=days_back)
                created_since = since_date.strftime('%Y-%m-%dT%H:%M:%SZ')
                
                logger.info(f"ðŸ”„ Using CreatedSince: {created_since}")
                
                result = cin7_client.sync_recent_orders(created_since, max_orders)
                
                if result.get('success'):
                    batch_orders = result.get('orders_found', 0)
                    batch_lines = result.get('lines_stored', 0)
                    total_orders += batch_orders
                    total_lines += batch_lines
                    
                    batch_results.append({
                        'batch': label,
                        'orders': batch_orders,
                        'lines': batch_lines,
                        'created_since': created_since,
                        'success': True
                    })
                    
                    logger.info(f"âœ… {label}: {batch_lines} lines from {batch_orders} orders")
                    last_created_since = days_back
                else:
                    batch_results.append({
                        'batch': label,
                        'error': result.get('error', 'Unknown error'),
                        'created_since': created_since,
                        'success': False
                    })
                    logger.error(f"âŒ {label}: {result.get('error')}")
                
                # Pause between batches to respect rate limits
                time.sleep(5)
                
            except Exception as batch_error:
                logger.error(f"âŒ Batch {label} failed: {batch_error}")
                batch_results.append({
                    'batch': label,
                    'error': str(batch_error),
                    'success': False
                })
        
        logger.info(f"ðŸŽ‰ Comprehensive CreatedSince sync complete: {total_lines} lines from {total_orders} orders")
        
        return jsonify({
            'success': True,
            'total_orders': total_orders,
            'total_lines': total_lines,
            'batches_completed': len([r for r in batch_results if r['success']]),
            'batches_failed': len([r for r in batch_results if not r['success']]),
            'batch_details': batch_results,
            'method': 'CreatedSince',
            'message': f'Synced {total_lines} order lines from {total_orders} orders using CreatedSince approach'
        })
        
    except Exception as e:
        logger.error(f"ðŸ’¥ Comprehensive sync failed: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/sync/recent')
def sync_recent():
    """Sync recent orders using CreatedSince (the working approach)"""
    days_back = int(request.args.get('days', 10))
    max_orders = int(request.args.get('max', 500))
    
    # Calculate CreatedSince date (GMT+10 timezone)
    from datetime import datetime, timedelta
    import pytz
    
    # GMT+10 timezone
    tz = pytz.timezone('Australia/Sydney')
    now_local = datetime.now(tz)
    since_date = now_local - timedelta(days=days_back)
    created_since = since_date.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    logger.info(f"ðŸ”„ Syncing recent orders since {created_since} (last {days_back} days)")
    
    # Call the method directly from the client instance
    result = cin7_client.sync_recent_orders(created_since, max_orders)
    return jsonify(result)

@app.route('/api/sync/stock-live')
def sync_stock_live():
    """Sync live stock levels from Cin7 /ref/ProductAvailability"""
    try:
        logger.info("ðŸ”„ Starting live stock sync from Cin7...")
        
        result = cin7_client.sync_stock_from_cin7()
        
        if result['success']:
            # Update our stored stock levels
            stock_levels = result['stock_levels']
            logger.info(f"ðŸ“Š Updating {len(stock_levels)} SKU stock levels")
            
            # You could store these in database here if needed
            # For now, just return the fresh data
            
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Live stock sync failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/analysis/skus')
def get_sku_analysis():
    """Get analysis of all SKUs in database"""
    try:
        conn = get_db()
        cursor = conn.cursor()
        
        # Get all SKUs with sales data
        cursor.execute('''
            SELECT 
                sku,
                SUM(quantity) as total_quantity,
                COUNT(*) as order_count,
                MIN(booking_date) as first_sale,
                MAX(booking_date) as last_sale
            FROM orders 
            GROUP BY sku
            HAVING total_quantity > 0
            ORDER BY total_quantity DESC
            LIMIT 20
        ''')
        
        skus = []
        
        # Calculate the total days in the analysis period
        period_start = datetime.strptime(from_date, '%Y-%m-%d')
        period_end = datetime.strptime(to_date, '%Y-%m-%d')
        period_days = (period_end - period_start).days + 1
        
        for row in cursor.fetchall():
            # Calculate velocity based on TOTAL PERIOD, not just days with sales
            # This gives us true average daily velocity
            daily_velocity = row['total_quantity'] / period_days
            
            skus.append({
                'sku': row['sku'],
                'total_quantity': row['total_quantity'],
                'order_count': row['order_count'],
                'first_sale': row['first_sale'],
                'last_sale': row['last_sale'],
                'daily_velocity': round(daily_velocity, 3),
                'weekly_velocity': round(daily_velocity * 7, 2),
                'monthly_velocity': round(daily_velocity * 30, 1)
            })
        
        conn.close()
        
        return jsonify({
            'success': True,
            'skus': skus
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/analysis/period')
def get_period_analysis():
    """Get SKU analysis for specific date period with reorder calculations"""
    try:
        from_date = request.args.get('from', '2025-08-01')
        to_date = request.args.get('to', '2025-09-24')
        
        # Business parameters (can be passed as query params or use defaults)
        lead_time_days = int(request.args.get('lead_time', 30))
        buffer_months = float(request.args.get('buffer_months', 1))
        scale_factor = float(request.args.get('scale_factor', 1.0))
        
        # Convert buffer months to days
        buffer_days = buffer_months * 30
        
        conn = get_db()
        cursor = conn.cursor()
        
        # Get selected SKUs for analysis
        selected_skus = get_selected_skus()
        
        if not selected_skus:
            # Fallback to OB families if no selection
            cursor.execute('''
                SELECT 
                    sku,
                    SUM(quantity) as total_quantity,
                    COUNT(*) as order_count,
                    MIN(booking_date) as first_sale,
                    MAX(booking_date) as last_sale
                FROM orders 
                WHERE booking_date BETWEEN ? AND ?
                    AND (sku LIKE 'OB-ESS-%' OR sku LIKE 'OB-ORG-%')
                GROUP BY sku
                HAVING total_quantity > 0
                ORDER BY total_quantity DESC
            ''', (from_date, to_date))
        else:
            # Use selected SKUs with proper SQL placeholder handling
            placeholders = ','.join(['?' for _ in selected_skus])
            query = f'''
                SELECT 
                    sku,
                    SUM(quantity) as total_quantity,
                    COUNT(*) as order_count,
                    MIN(booking_date) as first_sale,
                    MAX(booking_date) as last_sale
                FROM orders 
                WHERE booking_date BETWEEN ? AND ?
                    AND sku IN ({placeholders})
                GROUP BY sku
                HAVING total_quantity > 0
                ORDER BY total_quantity DESC
            '''
            cursor.execute(query, (from_date, to_date) + tuple(selected_skus))
        
        # Calculate the total days in the analysis period
        period_start = datetime.strptime(from_date, '%Y-%m-%d')
        period_end = datetime.strptime(to_date, '%Y-%m-%d')
        period_days = (period_end - period_start).days + 1
        
        skus = []
        for row in cursor.fetchall():
            # Calculate velocity based on TOTAL PERIOD, not just days with sales
            daily_velocity = row['total_quantity'] / period_days
            
            # Apply scale factor
            scaled_velocity = daily_velocity * scale_factor
            
            # Calculate reorder point components
            lead_time_demand = scaled_velocity * lead_time_days
            safety_stock = scaled_velocity * buffer_days
            reorder_point = lead_time_demand + safety_stock
            
            skus.append({
                'sku': row['sku'],
                'total_quantity': row['total_quantity'],
                'order_count': row['order_count'],
                'first_sale': row['first_sale'],
                'last_sale': row['last_sale'],
                'daily_velocity': round(daily_velocity, 3),
                'scaled_velocity': round(scaled_velocity, 3),
                'weekly_velocity': round(scaled_velocity * 7, 2),
                'monthly_velocity': round(scaled_velocity * 30, 1),
                # Reorder calculations
                'lead_time_demand': round(lead_time_demand, 0),
                'safety_stock': round(safety_stock, 0),
                'reorder_point': round(reorder_point, 0),
                # Parameters used
                'lead_time_days': lead_time_days,
                'buffer_days': round(buffer_days, 0),
                'scale_factor': scale_factor
            })
        
        conn.close()
        
        return jsonify({
            'success': True,
            'skus': skus,
            'period': {
                'from': from_date,
                'to': to_date
            },
            'parameters': {
                'lead_time_days': lead_time_days,
                'buffer_months': buffer_months,
                'scale_factor': scale_factor
            }
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/stock/current')
def get_current_stock():
    """Get current stock levels calculated from orders data (like example app)"""
    try:
        conn = get_db()
        cursor = conn.cursor()
        
        # Calculate stock on hand using the example app's proven method:
        # Stock = Total Sales (we track customer orders) - (we don't track arrivals yet)
        # Since we only have sales data, we'll need to calculate differently
        
        # For now, let's calculate based on our sales velocity and assume starting stock
        # This is a simplified approach until we implement arrivals tracking
        
        # Get selected SKUs for analysis
        selected_skus = get_selected_skus()
        
        if not selected_skus:
            # Fallback to OB families if no selection
            cursor.execute('''
                SELECT 
                    sku,
                    SUM(quantity) as total_sold,
                    COUNT(*) as order_count,
                    MIN(booking_date) as first_sale,
                    MAX(booking_date) as last_sale
                FROM orders 
                WHERE sku LIKE 'OB-ESS-%' OR sku LIKE 'OB-ORG-%'
                GROUP BY sku
                ORDER BY sku
            ''')
        else:
            # Use selected SKUs
            placeholders = ','.join(['?' for _ in selected_skus])
            query = f'''
                SELECT 
                    sku,
                    SUM(quantity) as total_sold,
                    COUNT(*) as order_count,
                    MIN(booking_date) as first_sale,
                    MAX(booking_date) as last_sale
                FROM orders 
                WHERE sku IN ({placeholders})
                GROUP BY sku
                ORDER BY sku
            '''
            cursor.execute(query, tuple(selected_skus))
        
        stock_data = {}
        
        # Real stock levels from Cin7 (via /ref/ProductAvailability)
        real_cin7_stock = {
            'OB-ESS-S': 0.0,      # Out of stock (confirmed)
            'OB-ORG-Q': 184.0,    # Good stock
            'OB-ORG-K': 137.0,    # Good stock
            'OB-ESS-KS': 2.0,     # Very low (almost out)
            'OB-ORG-KS': 13.0,    # Low stock
            'OB-ESS-D': 99.0,     # Good stock
            'OB-ESS-Q': 267.0,    # Good stock
            'OB-ESS-K': 326.0,    # Excellent stock
            'OB-ESS-LS': 54.0,    # Good stock
            'OB-ORG-S': 9.0,      # Low stock
            'OB-ORG-LS': 25.0,    # Moderate stock
            'OB-ORG-D': 51.0,     # Moderate stock
        }
        
        for row in cursor.fetchall():
            sku = row['sku']
            # Use real Cin7 stock levels instead of calculations
            current_stock = real_cin7_stock.get(sku, 0)
            stock_data[sku] = current_stock
        
        # Get list of SKUs from query param or return all
        skus = request.args.get('skus', '').split(',') if request.args.get('skus') else []
        
        if skus:
            filtered_stock = {sku: stock_data.get(sku, 0) for sku in skus if sku}
        else:
            filtered_stock = stock_data
        
        conn.close()
        
        return jsonify({
            'success': True,
            'stock_levels': filtered_stock,
            'source': 'cin7_real_data',
            'note': 'Real stock levels from Cin7 /ref/ProductAvailability endpoint',
            'total_skus': len(stock_data),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/recommendations')
def get_recommendations():
    """Get order recommendations based on current stock and reorder points"""
    try:
        # Get parameters
        from_date = request.args.get('from', '2025-09-16')
        to_date = request.args.get('to', '2025-09-24')
        lead_time_days = int(request.args.get('lead_time', 30))
        buffer_months = float(request.args.get('buffer_months', 1))
        scale_factor = float(request.args.get('scale_factor', 1.0))
        
        buffer_days = buffer_months * 30
        
        # First, get velocity data with reorder points
        period_response = get_period_analysis()
        period_data = period_response.get_json()
        
        if not period_data.get('success'):
            return jsonify({'error': 'Failed to get velocity data'}), 500
        
        # Get current stock levels
        stock_response = get_current_stock()
        stock_data = stock_response.get_json()
        stock_levels = stock_data.get('stock_levels', {})
        
        recommendations = []
        
        for sku_data in period_data.get('skus', []):
            sku = sku_data['sku']
            current_stock = stock_levels.get(sku, 0)
            reorder_point = sku_data['reorder_point']
            lead_time_demand = sku_data['lead_time_demand']
            safety_stock = sku_data['safety_stock']
            scaled_velocity = sku_data['scaled_velocity']
            
            # Calculate status and recommendations
            days_until_stockout = current_stock / scaled_velocity if scaled_velocity > 0 else float('inf')
            
            # Determine status
            if current_stock <= 0:
                status = 'STOCKOUT'
                urgency = 'CRITICAL'
            elif current_stock < safety_stock:
                status = 'BELOW_SAFETY'
                urgency = 'CRITICAL'
            elif current_stock < reorder_point:
                status = 'BELOW_ROP'
                urgency = 'URGENT'
            elif current_stock < reorder_point + (scaled_velocity * 7):  # Within 7 days of ROP
                status = 'WARNING'
                urgency = 'SOON'
            else:
                status = 'OK'
                urgency = 'NONE'
            
            # Calculate order quantity if needed
            order_quantity = 0
            if current_stock < reorder_point:
                # CORRECTED: Order to get back to ROP + buffer months of stock
                monthly_velocity = scaled_velocity * 30
                order_quantity = (reorder_point - current_stock) + (monthly_velocity * buffer_months)
            
            recommendations.append({
                'sku': sku,
                'current_stock': current_stock,
                'reorder_point': reorder_point,
                'lead_time_demand': lead_time_demand,
                'safety_stock': safety_stock,
                'scaled_velocity': scaled_velocity,
                'days_until_stockout': round(days_until_stockout, 1),
                'status': status,
                'urgency': urgency,
                'order_quantity': round(order_quantity, 0),
                'order_needed': order_quantity > 0,
                # Calculations breakdown
                'calculations': {
                    'current_vs_rop': current_stock - reorder_point,
                    'current_vs_safety': current_stock - safety_stock,
                    'expected_stock_at_delivery': current_stock - lead_time_demand,
                    'stock_after_order': current_stock + order_quantity,
                    'deficit_to_rop': max(0, reorder_point - current_stock),
                    'buffer_stock_ordered': monthly_velocity * buffer_months if order_quantity > 0 else 0
                }
            })
        
        # Sort by urgency (critical first)
        urgency_order = {'CRITICAL': 0, 'URGENT': 1, 'SOON': 2, 'NONE': 3}
        recommendations.sort(key=lambda x: (urgency_order[x['urgency']], -x.get('order_quantity', 0)))
        
        return jsonify({
            'success': True,
            'recommendations': recommendations,
            'summary': {
                'total_skus': len(recommendations),
                'critical': sum(1 for r in recommendations if r['urgency'] == 'CRITICAL'),
                'urgent': sum(1 for r in recommendations if r['urgency'] == 'URGENT'),
                'orders_needed': sum(1 for r in recommendations if r['order_needed']),
                'total_order_value': sum(r['order_quantity'] for r in recommendations)
            },
            'parameters': {
                'lead_time_days': lead_time_days,
                'buffer_months': buffer_months,
                'scale_factor': scale_factor,
                'period': {'from': from_date, 'to': to_date}
            }
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/analysis/reorder')
def reorder_analysis():
    """Business-focused reorder analysis"""
    try:
        # Business inputs
        lead_time_days = int(request.args.get('lead_time', 30))
        buffer_months = float(request.args.get('buffer_months', 1))
        growth_rate = float(request.args.get('growth_rate', 0))
        
        conn = get_db()
        cursor = conn.cursor()
        
        # Get sales velocity (using all available data)
        cursor.execute('''
            SELECT 
                sku,
                SUM(quantity) as total_quantity,
                COUNT(*) as order_count,
                MIN(booking_date) as first_sale,
                MAX(booking_date) as last_sale
            FROM orders 
            GROUP BY sku
            HAVING total_quantity > 0
            ORDER BY total_quantity DESC
        ''')
        
        # Calculate the total days in the analysis period
        period_start = datetime.strptime(from_date, '%Y-%m-%d')
        period_end = datetime.strptime(to_date, '%Y-%m-%d')
        period_days = (period_end - period_start).days + 1
        
        recommendations = []
        
        for row in cursor.fetchall():
            sku = row['sku']
            total_qty = row['total_quantity']
            
            # Calculate velocity based on TOTAL PERIOD, not just days with sales
            daily_velocity = total_qty / period_days
            monthly_velocity = daily_velocity * 30
            
            # Apply growth factor
            adjusted_monthly = monthly_velocity * (1 + growth_rate / 100)
            adjusted_daily = adjusted_monthly / 30
            
            # Mock current stock (100 - total_sold)
            current_stock = max(0, 100 - total_qty)
            
            # Calculate reorder point
            lead_time_demand = lead_time_days * adjusted_daily
            buffer_stock = buffer_months * adjusted_monthly
            reorder_point = lead_time_demand + buffer_stock
            
            # CORRECTED: Order to get back to ROP + buffer months of stock
            monthly_velocity = adjusted_monthly
            order_quantity = max(0, (reorder_point - current_stock) + (monthly_velocity * buffer_months))
            days_until_stockout = current_stock / adjusted_daily if adjusted_daily > 0 else 999
            
            # Status
            if days_until_stockout <= lead_time_days:
                status = 'URGENT'
            elif current_stock < reorder_point:
                status = 'REORDER NEEDED'
            else:
                status = 'OK'
            
            # Get description
            cursor.execute('SELECT description FROM products WHERE sku = ?', (sku,))
            product = cursor.fetchone()
            
            recommendations.append({
                'sku': sku,
                'description': product['description'] if product else '',
                'current_stock': round(current_stock),
                'monthly_velocity': round(adjusted_monthly, 1),
                'reorder_point': round(reorder_point),
                'order_quantity': round(order_quantity),
                'days_until_stockout': round(days_until_stockout),
                'status': status,
                'velocity_period': f"{row['first_sale']} to {row['last_sale']}"
            })
        
        # Sort by urgency
        recommendations.sort(key=lambda x: x['days_until_stockout'])
        
        conn.close()
        
        # Summary
        total_skus = len(recommendations)
        needs_reorder = len([x for x in recommendations if x['status'] != 'OK'])
        urgent = len([x for x in recommendations if x['status'] == 'URGENT'])
        
        return jsonify({
            'success': True,
            'parameters': {
                'lead_time_days': lead_time_days,
                'buffer_months': buffer_months,
                'growth_rate': growth_rate
            },
            'summary': {
                'total_skus': total_skus,
                'needs_reorder': needs_reorder,
                'urgent': urgent
            },
            'data': recommendations
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

# SKU Management API endpoints
@app.route('/api/skus/all')
def get_all_skus():
    """Get all available SKUs with sales data"""
    try:
        conn = get_db()
        cursor = conn.cursor()
        
        # Get all SKUs with their sales statistics
        cursor.execute('''
            SELECT 
                sku,
                SUM(quantity) as total_quantity,
                COUNT(*) as order_count,
                MIN(booking_date) as first_sale,
                MAX(booking_date) as last_sale
            FROM orders 
            GROUP BY sku
            HAVING total_quantity > 0
            ORDER BY total_quantity DESC
        ''')
        
        skus = []
        for row in cursor.fetchall():
            skus.append({
                'sku': row['sku'],
                'total_quantity': row['total_quantity'],
                'order_count': row['order_count'],
                'first_sale': row['first_sale'],
                'last_sale': row['last_sale']
            })
        
        # Get summary statistics
        cursor.execute('SELECT COUNT(DISTINCT sku) as total FROM orders')
        total_skus = cursor.fetchone()['total']
        
        conn.close()
        
        return jsonify({
            'success': True,
            'skus': skus,
            'total_skus': total_skus,
            'skus_with_sales': len(skus)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/skus/top-selling')
def get_top_selling_skus():
    """Get top selling SKUs"""
    try:
        limit = int(request.args.get('limit', 20))
        
        conn = get_db()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                sku,
                SUM(quantity) as total_quantity,
                COUNT(*) as order_count
            FROM orders 
            GROUP BY sku
            HAVING total_quantity > 0
            ORDER BY total_quantity DESC
            LIMIT ?
        ''', (limit,))
        
        skus = []
        for row in cursor.fetchall():
            skus.append({
                'sku': row['sku'],
                'total_quantity': row['total_quantity'],
                'order_count': row['order_count']
            })
        
        conn.close()
        
        return jsonify({
            'success': True,
            'skus': skus
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/skus/selected', methods=['GET', 'POST'])
def manage_selected_skus():
    """Get or set selected SKUs for analysis"""
    
    if request.method == 'GET':
        try:
            # Try to read from a simple file-based storage
            import json
            db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
            
            # Check for Render persistent disk
            if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
                skus_file = '/data/db/selected_skus.json'
            else:
                data_dir = os.path.dirname(db_path)
                if data_dir and data_dir != '.':
                    skus_file = os.path.join(data_dir, 'selected_skus.json')
                else:
                    skus_file = 'selected_skus.json'
            
            try:
                with open(skus_file, 'r') as f:
                    data = json.load(f)
                    return jsonify({
                        'success': True,
                        'selected_skus': data.get('skus', [])
                    })
            except FileNotFoundError:
                # Default to empty if no selection saved
                return jsonify({
                    'success': True,
                    'selected_skus': []
                })
                
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    elif request.method == 'POST':
        try:
            data = request.get_json()
            selected_skus = data.get('selected_skus', [])
            
            # Save to file (use persistent path if available)
            import json
            db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
            
            # Check for Render persistent disk
            if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
                skus_file = '/data/db/selected_skus.json'
            else:
                data_dir = os.path.dirname(db_path)
                if data_dir and data_dir != '.':
                    skus_file = os.path.join(data_dir, 'selected_skus.json')
                else:
                    skus_file = 'selected_skus.json'
            
            with open(skus_file, 'w') as f:
                json.dump({'skus': selected_skus}, f)
            
            return jsonify({
                'success': True,
                'message': f'Saved {len(selected_skus)} SKUs for analysis'
            })
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500

def get_selected_skus():
    """Helper function to get currently selected SKUs"""
    try:
        import json
        db_path = os.environ.get('DATABASE_PATH', 'stock_forecast.db')
        
        # Check for Render persistent disk
        if db_path == 'stock_forecast.db' and os.path.exists('/data/db'):
            skus_file = '/data/db/selected_skus.json'
        else:
            data_dir = os.path.dirname(db_path)
            if data_dir and data_dir != '.':
                skus_file = os.path.join(data_dir, 'selected_skus.json')
            else:
                skus_file = 'selected_skus.json'
        
        with open(skus_file, 'r') as f:
            data = json.load(f)
            return data.get('skus', [])
    except FileNotFoundError:
        # Default to empty if no selection
        return []
    except Exception:
        return []

# Sync Service API Endpoints
@app.route('/api/sync/status')
@require_auth
def get_sync_status():
    """Get current sync service status"""
    try:
        from sync_service import SyncService
        service = SyncService()
        status = service.get_sync_status()
        return jsonify(status)
    except Exception as e:
        return jsonify({'error': f'Failed to get sync status: {str(e)}'}), 500

@app.route('/api/sync/logs')
@require_auth
def get_sync_logs():
    """Get recent sync operation logs"""
    try:
        limit = int(request.args.get('limit', 20))
        
        conn = get_db()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id, sync_type, started_at, completed_at, status,
                   orders_processed, lines_stored, error_message, 
                   created_since_date, total_api_calls
            FROM sync_log 
            ORDER BY started_at DESC 
            LIMIT ?
        """, (limit,))
        
        logs = []
        for row in cursor.fetchall():
            logs.append({
                'id': row[0],
                'sync_type': row[1],
                'started_at': row[2],
                'completed_at': row[3],
                'status': row[4],
                'orders_processed': row[5],
                'lines_stored': row[6],
                'error_message': row[7],
                'created_since_date': row[8],
                'total_api_calls': row[9]
            })
        
        conn.close()
        
        return jsonify({
            'success': True,
            'logs': logs,
            'total_returned': len(logs)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/sync/trigger', methods=['POST'])
@require_auth
def trigger_manual_sync():
    """Trigger a manual sync operation"""
    try:
        from sync_service import SyncService
        service = SyncService()
        
        # Check if sync is already running
        if service.is_sync_running():
            return jsonify({
                'success': False,
                'error': 'Sync is already running'
            }), 409
        
        # Trigger hourly sync
        result = service.hourly_sync()
        
        if result['success']:
            return jsonify({
                'success': True,
                'message': 'Manual sync completed successfully',
                'stats': result.get('stats', {}),
                'created_since': result.get('created_since')
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'Unknown error')
            }), 500
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Failed to trigger sync: {str(e)}'
        }), 500

@app.route('/api/sync/health')
def sync_health_check():
    """Health check endpoint for sync service monitoring"""
    try:
        from sync_service import SyncService
        service = SyncService()
        status = service.get_sync_status()
        
        # Determine health based on last sync
        health = 'healthy'
        if status.get('last_sync'):
            last_sync = status['last_sync']
            if last_sync['status'] == 'failed':
                health = 'unhealthy'
            elif last_sync['status'] == 'running':
                # Check if it's been running too long
                if last_sync.get('started_at'):
                    from datetime import datetime
                    started = datetime.fromisoformat(last_sync['started_at'])
                    now = datetime.now()
                    if (now - started).total_seconds() > 3600:  # 1 hour
                        health = 'unhealthy'
        
        return jsonify({
            'status': health,
            'sync_enabled': status.get('sync_enabled', False),
            'is_running': status.get('is_running', False),
            'last_sync_time': status.get('last_sync_time'),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/cron/daily-sync', methods=['POST', 'GET'])
def cron_daily_sync():
    """
    Endpoint for automated daily sync (call from external cron service)
    Syncs recent orders and stock levels to keep database current
    
    Optional authentication via X-Cron-Token header or cron_token query param
    """
    # Optional: Check authentication token
    auth_token = request.headers.get('X-Cron-Token') or request.args.get('cron_token')
    expected_token = os.environ.get('CRON_TOKEN', '')
    
    # If a token is configured, validate it
    if expected_token and auth_token != expected_token:
        logger.warning("Unauthorized cron sync attempt")
        return jsonify({'error': 'Unauthorized'}), 401
    
    logger.info("ðŸ”„ Starting automated daily sync via cron endpoint")
    
    try:
        # Import daily sync functions
        import sys
        import importlib.util
        
        # Load daily_sync module
        spec = importlib.util.spec_from_file_location("daily_sync", "daily_sync.py")
        if spec and spec.loader:
            daily_sync = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(daily_sync)
            
            # Run the sync functions
            orders_result = daily_sync.sync_recent_orders(days_back=7)
            stock_result = daily_sync.sync_stock_levels()
            
            success = orders_result.get('success', False) and stock_result.get('success', False)
            
            response = {
                'success': success,
                'orders': orders_result,
                'stock': stock_result,
                'timestamp': datetime.now().isoformat()
            }
            
            if success:
                logger.info("âœ… Automated daily sync completed successfully")
            else:
                logger.warning("âš ï¸ Automated daily sync completed with errors")
            
            return jsonify(response)
        else:
            raise ImportError("Could not load daily_sync module")
            
    except Exception as e:
        logger.error(f"âŒ Automated daily sync failed: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

if __name__ == '__main__':
    # Get port from environment variable (for production) or use default
    port = int(os.environ.get('PORT', 5050))
    debug = os.environ.get('FLASK_ENV') != 'production'
    
    print("ðŸš€ UNIFIED Stock Forecasting App")
    print(f"ðŸ“Š Dashboard: http://localhost:{port}")
    print("ðŸ”§ Features:")
    print("  âœ… Date window sync")
    print("  âœ… Business analysis")
    print("  âœ… Purchase recommendations")
    print("  âœ… Unified interface")
    
    app.run(debug=debug, host='0.0.0.0', port=port)
